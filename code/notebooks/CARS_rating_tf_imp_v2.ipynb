{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bpadmin\\anaconda3\\envs\\tensorflow_cuda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"timestamp\": x[\"timestamp\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"user_rating\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles = ratings.batch(1_000_000).map(lambda x: x[\"movie_title\"])\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "# need these to initialize timestamp embedding layers in future steps\n",
    "\n",
    "timestamps = np.concatenate(list(ratings.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
    "\n",
    "max_timestamp = timestamps.max()\n",
    "min_timestamp = timestamps.min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this handles embedding user Identifiers and contextual data.\n",
    "time stamp is used as the contexual information here.\n",
    "using timestamp is \n",
    "'''\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        use_timestamp,\n",
    "        unique_user_ids, \n",
    "        timestamps, \n",
    "        timestamp_buckets):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_timestamp = use_timestamp\n",
    "        self.unique_user_ids = unique_user_ids\n",
    "        self.timestamp_buckets = timestamp_buckets\n",
    "        self.timestamps = timestamps\n",
    "        \n",
    "        self.embed_user_id = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary = self.unique_user_ids,\n",
    "                mask_token = None\n",
    "            ),\n",
    "            tf.keras.layers.Embedding(\n",
    "                input_dim = len(self.unique_user_ids)+1,\n",
    "                output_dim = 32\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        if self.use_timestamp:\n",
    "            self.embed_timestamp = tf.keras.Sequential([\n",
    "                tf.keras.layers.Discretization(\n",
    "                    bin_boundaries = list(self.timestamp_buckets)\n",
    "                ),\n",
    "\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim = len(list(self.timestamp_buckets))+1 ,\n",
    "                    output_dim = 32\n",
    "                )\n",
    "            ])\n",
    "\n",
    "            self.normalize_timestamp = tf.keras.layers.Normalization(\n",
    "                axis = None #calcuate a scaler mean and variance \n",
    "            )\n",
    "            self.normalize_timestamp.adapt(self.timestamps)\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        user_id, timestamp = inputs\n",
    "\n",
    "        if self.use_timestamp:\n",
    "            user_id_embed = self.embed_user_id(user_id)\n",
    "            timestamp_embed = self.embed_timestamp(timestamp)\n",
    "            norm_timestamp = tf.reshape(self.normalize_timestamp(timestamp), (-1,1)) #(-1,1) means first dimension to be infered\n",
    "\n",
    "            return tf.concat([user_id_embed, timestamp_embed, norm_timestamp], axis = 1) #concatenate vertically\n",
    "            \n",
    "        return self.embed_user_id(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this handles embedding item Identifiers and contextual data.\n",
    "movie title itself is used as the contexual information here.\n",
    "using timestamp is \n",
    "'''\n",
    "\n",
    "class ItemModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unique_movie_titles,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_tokens = 10000\n",
    "        self.unique_movie_titles\n",
    "\n",
    "        self.embed_item_id = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary = self.unique_movie_titles,\n",
    "                mask_token =None\n",
    "            ),\n",
    "            tf.keras.layers.Embedding(\n",
    "                input_dim = len(self.unique_movie_titles)+1,\n",
    "                output_dim = 32\n",
    "            )\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.textvectorizer = tf.keras.layers.TextVectorization(\n",
    "            max_tokens = self.max_tokens\n",
    "        )\n",
    "\n",
    "        self.embed_item_title = tf.keras.Sequential([\n",
    "            self.textvectorizer,\n",
    "\n",
    "            tf.keras.layers.Embedding(\n",
    "                input_dim = self.max_tokens,\n",
    "                output_dim = 32,\n",
    "                mask_zero = True\n",
    "            ),\n",
    "\n",
    "            tf.keras.layers.GlobalAveragePooling1D() # reduces dimensionality to 1d (embedding layer embeddeds each word in a title one by one)\n",
    "        ])\n",
    "\n",
    "        self.textvectorizer.adapt(self.unique_movie_titles)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        movie_title = inputs\n",
    "\n",
    "        return tf.concat([\n",
    "            self.embed_item_id(movie_title),\n",
    "            self.embed_item_title(movie_title)\n",
    "        ],\n",
    "        axis = 1)\n",
    "        \n",
    "        # return self.embed_item_title(inputs['movie_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(\n",
    "\n",
    "    self,\n",
    "    use_timestamp,\n",
    "    unique_user_ids, \n",
    "    timestamps, \n",
    "    timestamp_buckets,\n",
    "    unique_movie_titles\n",
    "    ):\n",
    "    \n",
    "    super().__init__()\n",
    "\n",
    "    # embedding_dimension = 32\n",
    "    self.use_timestamp = use_timestamp\n",
    "    self.unique_user_ids = unique_user_ids, \n",
    "    self.timestamps = timestamps, \n",
    "    self.timestamp_buckets = timestamp_buckets,\n",
    "    self.unique_movie_titles = unique_movie_titles\n",
    "\n",
    "    self.user_embeddings = UserModel(\n",
    "      use_timestamp = self.use_timestamp,\n",
    "      unique_user_ids = self.unique_user_ids, \n",
    "      timestamps = self.timestamps, \n",
    "      timestamp_buckets = self.timestamp_buckets\n",
    "      )\n",
    "\n",
    "    self.movie_embeddings = ItemModel(\n",
    "      unique_movie_titles = self.unique_movie_titles\n",
    "      )\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.ratings = tf.keras.Sequential([\n",
    "      # Learn multiple dense layers.\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      # Make rating predictions in the final layer.\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    user_id, timestamp, movie_title = inputs\n",
    "\n",
    "    user_embedding = self.user_embeddings((user_id,timestamp))\n",
    "    movie_embedding = self.movie_embeddings(movie_title)\n",
    "\n",
    "    return self.ratings(tf.concat([user_embedding, movie_embedding], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_model = UserModel(\n",
    "    use_timestamp = True,\n",
    "    unique_user_ids = unique_user_ids, \n",
    "    timestamps = timestamps, \n",
    "    timestamp_buckets = timestamp_buckets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=['42']. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=['42']. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=[<tf.Tensor: shape=(), dtype=int32, numpy=886479771>]. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=[<tf.Tensor: shape=(), dtype=int32, numpy=886479771>]. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 65), dtype=float32, numpy=\n",
       "array([[-0.04591095, -0.01999645, -0.04861705, -0.03927895,  0.02113768,\n",
       "         0.03359554,  0.03974316,  0.00665708, -0.03207498, -0.03659819,\n",
       "         0.0271732 ,  0.04544226, -0.02630298, -0.01789683,  0.0431136 ,\n",
       "        -0.00808169,  0.00355117, -0.0261207 ,  0.01582832, -0.0086633 ,\n",
       "         0.03367618, -0.03683843,  0.0203097 , -0.01870381, -0.04977077,\n",
       "         0.03804914,  0.02514446, -0.04109829, -0.04488961,  0.03018678,\n",
       "        -0.00188952,  0.03227441, -0.0071973 ,  0.04285849, -0.0406331 ,\n",
       "         0.04734926,  0.03733401, -0.01190649, -0.04161644,  0.0249857 ,\n",
       "        -0.02515309,  0.04760442, -0.03168219,  0.02971799, -0.0162881 ,\n",
       "        -0.02007396, -0.00829492, -0.02891862, -0.02232406,  0.00701983,\n",
       "         0.00155596,  0.00110019, -0.04234785,  0.04778567, -0.02763255,\n",
       "         0.00759138,  0.04885993, -0.03452522, -0.02660987, -0.03732632,\n",
       "         0.02734119, -0.0113823 , -0.01996914, -0.03777488,  0.55233073]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_user_model(([\"42\"],[886479771]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ranking_model = RankingModel(\n",
    "    use_timestamp = True,\n",
    "    unique_user_ids = unique_user_ids, \n",
    "    timestamps = timestamps, \n",
    "    timestamp_buckets = timestamp_buckets,\n",
    "    unique_movie_titles = unique_movie_titles\n",
    "    )\n",
    "\n",
    "test_ranking_model(([\"42\"],[886479771], [\"One Flew Over the Cuckoo's Nest (1975)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    use_timestamp,\n",
    "    unique_user_ids, \n",
    "    timestamps, \n",
    "    timestamp_buckets,\n",
    "    unique_movie_titles\n",
    "    ):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.use_timestamp = use_timestamp\n",
    "    self.unique_user_ids = unique_user_ids, \n",
    "    self.timestamps = timestamps, \n",
    "    self.timestamp_buckets = timestamp_buckets,\n",
    "    self.unique_movie_titles = unique_movie_titles\n",
    "\n",
    "    self.ranking_model: tf.keras.Model = RankingModel(\n",
    "      use_timestamp = self.use_timestamp,\n",
    "      unique_user_ids = self.unique_user_ids, \n",
    "      timestamps = self.timestamps, \n",
    "      timestamp_buckets = self.timestamp_buckets,\n",
    "      unique_movie_titles = self.unique_movie_titles\n",
    "      )\n",
    "\n",
    "    self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "      loss = tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "\n",
    "  def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "    return self.ranking_model(\n",
    "        (features[\"user_id\"], features['timestamp'], features[\"movie_title\"]))\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    labels = features.pop(\"user_rating\")\n",
    "\n",
    "    rating_predictions = self(features)\n",
    "\n",
    "    return self.task(labels=labels, predictions=rating_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMovielensModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_timestamp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43munique_user_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munique_user_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp_buckets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtimestamp_buckets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43munique_movie_titles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munique_movie_titles\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdagrad(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[43], line 20\u001b[0m, in \u001b[0;36mMovielensModel.__init__\u001b[1;34m(self, use_timestamp, unique_user_ids, timestamps, timestamp_buckets, unique_movie_titles)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp_buckets \u001b[38;5;241m=\u001b[39m timestamp_buckets,\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_movie_titles \u001b[38;5;241m=\u001b[39m unique_movie_titles\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mranking_model: tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel \u001b[38;5;241m=\u001b[39m \u001b[43mRankingModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43muse_timestamp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_timestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m  \u001b[49m\u001b[43munique_user_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_user_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtimestamp_buckets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamp_buckets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m  \u001b[49m\u001b[43munique_movie_titles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_movie_titles\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask: tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLayer \u001b[38;5;241m=\u001b[39m tfrs\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mRanking(\n\u001b[0;32m     29\u001b[0m   loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanSquaredError(),\n\u001b[0;32m     30\u001b[0m   metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mRootMeanSquaredError()]\n\u001b[0;32m     31\u001b[0m )\n",
      "Cell \u001b[1;32mIn[33], line 22\u001b[0m, in \u001b[0;36mRankingModel.__init__\u001b[1;34m(self, use_timestamp, unique_user_ids, timestamps, timestamp_buckets, unique_movie_titles)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp_buckets \u001b[38;5;241m=\u001b[39m timestamp_buckets,\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_movie_titles \u001b[38;5;241m=\u001b[39m unique_movie_titles\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mUserModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m  \u001b[49m\u001b[43muse_timestamp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_timestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m  \u001b[49m\u001b[43munique_user_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_user_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtimestamp_buckets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamp_buckets\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovie_embeddings \u001b[38;5;241m=\u001b[39m ItemModel(\n\u001b[0;32m     30\u001b[0m   unique_movie_titles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_movie_titles\n\u001b[0;32m     31\u001b[0m   )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Compute predictions.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 23\u001b[0m, in \u001b[0;36mUserModel.__init__\u001b[1;34m(self, use_timestamp, unique_user_ids, timestamps, timestamp_buckets)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp_buckets \u001b[38;5;241m=\u001b[39m timestamp_buckets\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamps \u001b[38;5;241m=\u001b[39m timestamps\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_user_id \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStringLookup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munique_user_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     27\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(\n\u001b[0;32m     28\u001b[0m         input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(unique_user_ids)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     29\u001b[0m         output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     30\u001b[0m     )\n\u001b[0;32m     31\u001b[0m ])\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_timestamp:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_timestamp \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     35\u001b[0m         tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDiscretization(\n\u001b[0;32m     36\u001b[0m             bin_boundaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp_buckets)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m         )\n\u001b[0;32m     43\u001b[0m     ])\n",
      "File \u001b[1;32mc:\\Users\\bpadmin\\anaconda3\\envs\\tensorflow_cuda\\lib\\site-packages\\keras\\layers\\preprocessing\\string_lookup.py:334\u001b[0m, in \u001b[0;36mStringLookup.__init__\u001b[1;34m(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary, idf_weights, encoding, invert, output_mode, sparse, pad_to_max_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m encoding\n\u001b[1;32m--> 334\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    335\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[0;32m    336\u001b[0m     num_oov_indices\u001b[38;5;241m=\u001b[39mnum_oov_indices,\n\u001b[0;32m    337\u001b[0m     mask_token\u001b[38;5;241m=\u001b[39mmask_token,\n\u001b[0;32m    338\u001b[0m     oov_token\u001b[38;5;241m=\u001b[39moov_token,\n\u001b[0;32m    339\u001b[0m     vocabulary\u001b[38;5;241m=\u001b[39mvocabulary,\n\u001b[0;32m    340\u001b[0m     vocabulary_dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mstring,\n\u001b[0;32m    341\u001b[0m     idf_weights\u001b[38;5;241m=\u001b[39midf_weights,\n\u001b[0;32m    342\u001b[0m     invert\u001b[38;5;241m=\u001b[39minvert,\n\u001b[0;32m    343\u001b[0m     output_mode\u001b[38;5;241m=\u001b[39moutput_mode,\n\u001b[0;32m    344\u001b[0m     sparse\u001b[38;5;241m=\u001b[39msparse,\n\u001b[0;32m    345\u001b[0m     pad_to_max_tokens\u001b[38;5;241m=\u001b[39mpad_to_max_tokens,\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    348\u001b[0m base_preprocessing_layer\u001b[38;5;241m.\u001b[39mkeras_kpl_gauge\u001b[38;5;241m.\u001b[39mget_cell(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStringLookup\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mset(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    350\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bpadmin\\anaconda3\\envs\\tensorflow_cuda\\lib\\site-packages\\keras\\layers\\preprocessing\\index_lookup.py:319\u001b[0m, in \u001b[0;36mIndexLookup.__init__\u001b[1;34m(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary_dtype, vocabulary, idf_weights, invert, output_mode, sparse, pad_to_max_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf_weights_const \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf_weights\u001b[38;5;241m.\u001b[39mvalue()\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vocabulary \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midf_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# When restoring from a keras SavedModel, the loading code will\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# expect to find and restore a lookup_table attribute on the layer.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# This table needs to be uninitialized as a StaticHashTable cannot\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# be initialized twice.\u001b[39;00m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uninitialized_lookup_table()\n",
      "File \u001b[1;32mc:\\Users\\bpadmin\\anaconda3\\envs\\tensorflow_cuda\\lib\\site-packages\\keras\\layers\\preprocessing\\index_lookup.py:525\u001b[0m, in \u001b[0;36mIndexLookup.set_vocabulary\u001b[1;34m(self, vocabulary, idf_weights)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    523\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m vocabulary\n\u001b[1;32m--> 525\u001b[0m repeated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_repeated_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repeated_tokens:\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    528\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe passed vocabulary has at least one repeated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterm. Please uniquify your dataset. The repeated terms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    530\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mare \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(repeated_tokens)\n\u001b[0;32m    531\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bpadmin\\anaconda3\\envs\\tensorflow_cuda\\lib\\site-packages\\keras\\layers\\preprocessing\\index_lookup.py:906\u001b[0m, in \u001b[0;36mIndexLookup._find_repeated_tokens\u001b[1;34m(self, vocabulary)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_find_repeated_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocabulary):\n\u001b[0;32m    905\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return all repeated tokens in a vocabulary.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 906\u001b[0m     vocabulary_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vocabulary) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocabulary_set):\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    909\u001b[0m             item\n\u001b[0;32m    910\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item, count \u001b[38;5;129;01min\u001b[39;00m collections\u001b[38;5;241m.\u001b[39mCounter(vocabulary)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    911\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    912\u001b[0m         ]\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "model = MovielensModel(\n",
    "    use_timestamp = True,\n",
    "    unique_user_ids = unique_user_ids, \n",
    "    timestamps = timestamps, \n",
    "    timestamp_buckets = timestamp_buckets,\n",
    "    unique_movie_titles = unique_movie_titles\n",
    "    )\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 14ms/step - root_mean_squared_error: 1.1122 - loss: 1.2325 - regularization_loss: 0.0000e+00 - total_loss: 1.2325\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 13ms/step - root_mean_squared_error: 1.1026 - loss: 1.2097 - regularization_loss: 0.0000e+00 - total_loss: 1.2097\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 13ms/step - root_mean_squared_error: 1.0815 - loss: 1.1632 - regularization_loss: 0.0000e+00 - total_loss: 1.1632\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 14ms/step - root_mean_squared_error: 1.0597 - loss: 1.1166 - regularization_loss: 0.0000e+00 - total_loss: 1.1166\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 13ms/step - root_mean_squared_error: 1.0398 - loss: 1.0753 - regularization_loss: 0.0000e+00 - total_loss: 1.0753\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 14ms/step - root_mean_squared_error: 1.0282 - loss: 1.0536 - regularization_loss: 0.0000e+00 - total_loss: 1.0536\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 14ms/step - root_mean_squared_error: 1.0244 - loss: 1.0426 - regularization_loss: 0.0000e+00 - total_loss: 1.0426\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 14ms/step - root_mean_squared_error: 0.9944 - loss: 0.9824 - regularization_loss: 0.0000e+00 - total_loss: 0.9824\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 14ms/step - root_mean_squared_error: 0.9750 - loss: 0.9452 - regularization_loss: 0.0000e+00 - total_loss: 0.9452\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 14ms/step - root_mean_squared_error: 0.9658 - loss: 0.9285 - regularization_loss: 0.0000e+00 - total_loss: 0.9285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2051c010520>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 25ms/step - root_mean_squared_error: 0.9756 - loss: 0.9494 - regularization_loss: 0.0000e+00 - total_loss: 0.9494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'root_mean_squared_error': 0.9756286144256592,\n",
       " 'loss': 0.9386831521987915,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 0.9386831521987915}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings:\n",
      "M*A*S*H (1970): [[3.9183755]]\n",
      "Dances with Wolves (1990): [[3.6211972]]\n",
      "Speed (1994): [[3.5145564]]\n"
     ]
    }
   ],
   "source": [
    "test_ratings = {}\n",
    "test_movie_titles = [\"M*A*S*H (1970)\", \"Dances with Wolves (1990)\", \"Speed (1994)\"]\n",
    "for movie_title in test_movie_titles:\n",
    "  test_ratings[movie_title] = model({\n",
    "      \"user_id\": np.array([\"42\"]),\n",
    "      \"timestamp\": np.array([886479771]),\n",
    "      \"movie_title\": np.array([movie_title])\n",
    "  })\n",
    "\n",
    "print(\"Ratings:\")\n",
    "for title, score in sorted(test_ratings.items(), key=lambda x: x[1], reverse=True):\n",
    "  print(f\"{title}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
